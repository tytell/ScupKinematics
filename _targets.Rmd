---
title: "Scup data processing"
output: html_document
date: "2023-12-08"
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

```{r}
library(targets)
library(tarchetypes)
library(tidyverse)
library(here)
library(arrow)
library(plotly)
```

# Setup

This removes the auto-generated `_targets.R` file.

```{r}
tar_unscript()
```

# Globals

We first define some global options/functions common to all targets. 

```{targets global-paths, tar_globals = TRUE}
library(tarchetypes)

options(tidyverse.quiet = TRUE)
tar_option_set(
  packages = c("tidyverse", "here", "R.utils", "arrow", "cli"),
  workspace_on_error = TRUE, # Save a workspace file for a target that errors out.
  format = "parquet"
)
tar_resources_parquet(compression = "snappy")

library(cli)
options(cli.progress_show_after = 0)
options(cli.progress_clear = FALSE)

```

```{targets global-defs, tar_globals = TRUE}
nfilesingroup <- 5

if (.Platform$OS.type == "unix") {
  videodatapath <- "/Volumes/Data/WHOI-2022/Data from Erik Anderson/TE_WHOI_2022_bottomview/experiments"
  datarootpath <- "/Volumes/DataSync/ScupKinematics/processed_data/experiments"
} else {
  videodatapath <- "Z:\\WHOI-2022\\Data from Erik Anderson\\TE_WHOI_2022_bottomview\\experiments"
  datarootpath <- "Y:\\ScupKinematics\\processed_data\\experiments"
}
rawdatapath <- "raw_data"
processeddatapath <- "processed_data"

s.frac <- pracma::linspace(0, 1, n = 26)
dt <- 0.02
```

```{targets functions, tar_globals = TRUE}
source("code/scan_raw_files.R")
source("code/midlines.R")
```

## Target to scan the data files

Quickly list off all the data files and put them into the big CSV file containing the file names, paths, and numbers of lines.

```{targets filelistfile}
list(
  tar_file(filelistfile, 
           scan_bottomview_files(videodatapath, 
                                 here(processeddatapath,
                                      "all_bottomview_files.csv"),
                                 
                                 # look at 20 files in interactive mode, but
                                 # do all of them in the pipeline
                                 nfiles = tar_toggle(20, Inf)))
)
```

```{r}
filelist <- read_csv(filelistfile)
head(filelist)
```

```{r}
filelist |> 
  group_by(id, speed_Hz) |> 
  summarize(n = sum(nrows > 0)) |> 
  filter(n > 0) |> 
  summarize(minspeed_Hz = min(speed_Hz),
            maxspeed_Hz = max(speed_Hz),
            nspeeds = sum(n > 0))
```

# Matlab processing

We need to do the 3D calibration and triangulation in Matlab, which doesn't
interface with R well.

`prompt_run_matlab` just prompts to run a Matlab script that produces an output file. 
If the output file is older than the matlab script, it fails, otherwise it succeeds.

`check_matlab_processing` checks the number of files listed as processed in
the output data file. It will succeed if more than 80\% are listed as processed.

This Matlab script will perform the 3D calibration.
```{targets calibration}
list(
  # Check that the Matlab script isn't changed
  tar_file(calibration_m, here("code", "bottomview_06_16_2022_calib.m")),
  
  # Prompt to run it and save the results in the calibration file
  tar_target(calibration, 
             prompt_run_matlab(calibration_m,
                               here("processed_data", 
                                    "TE_WHOI_2022_bottomview/calibrations",
                                    "TE_06_16_2022_1655_calibration.mat")),
             format = "file")
)
```

This Matlab script then applies the calibration to the 2D data to triangulate
the 3D coordinates.
```{targets triangulate}
list(
  # This is the Matlab script
  tar_file(triangulate_m, "code/triangulate_all_bottomview_data.m"),
  
  # and this checks that it's run, and what fraction of the data files are processed
  tar_target(data3d, 
             check_matlab_processing(triangulate_m,
                               here(processeddatapath, "processed_bottomview_files.csv"),
                               filelist, calibration),
             format = "file")
)
```

# Merge the processed data files

This target reads in the name of all of the individual data files and checks
if the 3D triangulated file exists. Then it filters to just the files where
we have 3D data.
```{targets getdatafiles}
list(
  tar_file_read(processedfiles, 
                here(processeddatapath, "processed_bottomview_files.csv"),
                read_csv(file = !!.x) |> 
                  mutate(fileexists = map_vec(data3dfile, 
                                              \(f) file.exists(file.path(datarootpath, f))),
                         .progress = "Finding data files")),
  
  tar_target(testfiles, 
             processedfiles |> 
               filter(isprocessed & fileexists) |> 
               
               # in interactive mode, choose 3 random fish
               filter(id %in% (
                 processedfiles |> 
                   ungroup() |> 
                   distinct(id) |> 
                   slice_sample(n = tar_toggle(3, Inf)) |> 
                   pull(id)
               )) |> 
               
               group_by(id) |> 
               
               # in interactive mode, take 3 random files per fish. Otherwise
               # take all of them
               slice_sample(n = tar_toggle(3, Inf)) |> 
               
               mutate(fishgroup = floor(seq(1,n()) / nfilesingroup))
  )
)
```
## Stats on the processed files

Range of speeds:
```{r}
filesummary <-
  processedfiles |> 
  group_by(id) |> 
  summarize(minspeed = min(speed_Hz),
            maxspeed = max(speed_Hz),
            nspeed = length(unique(speed_Hz)))

filesummary
```

Number of different fish and their sizes
```{r}
filesummary |> 
  ungroup() |> 
  summarize(nfish = length(unique(id)),
            across(where(is.numeric), list(mn = mean, sd = sd)))
```
If we limited the number of files, these are the files we'll be focusing on.
```{r}
testfiles |> 
  group_by(id) |> 
  summarize(minspeed = min(speed_Hz),
            maxspeed = max(speed_Hz),
            nspeed = length(unique(speed_Hz)))
  
```

## Do the big merge

### Process the individual data files

First group by each fish and, within each fish, every `nfilesingroup` file.

Then, within each group, read in each of the data files.

```{targets read_data_files}
list(
  tar_group_by(filegroup, testfiles, id, fishgroup),
  tar_target(outlinedata1,
             purrr::map_df(filegroup$data3dfile,
                    \(f) read_data_file(datarootpath, f)),
             pattern = map(filegroup),
             error = "continue")
)
```

### Plot the data

Coordinate system: $x$ is forward, $y$ is left, and $z$ is up.

```{r}
filenames <-
  outlinedata1 |> 
  ungroup() |> 
  distinct(filename) |> 
  pull(filename)
```

```{r}
d1 <-
  outlinedata1 |> 
  filter(filename == filenames[3],
         camera == "C1") |> 
  ungroup()

p1 <-
  plot_ly(d1, x = ~tsec, y = ~tailx, name = "x") |>
  add_lines() |>
  add_markers()

p2 <-
  plot_ly(d1, x = ~tsec, y = ~taily, name = "y") |>
  add_lines() |>
  add_markers()

p3 <-
  plot_ly(d1, x = ~tsec, y = ~tailz, name = "z") |>
  add_lines() |>
  add_markers()

subplot(p1, p2, p3, nrows = 3, shareX = TRUE)
```

```{r}
d1 |> 
  filter(tsec >= 14 & tsec <= 14.5) |> 
  unnest(outline) |> 
  group_by(imnum) |> 
  plot_ly(x = ~xctr + x.head, y = ~yctr + y.head, color = ~side) |> 
  add_lines() |> 
  add_markers(x = ~x.head, y = ~y.head) |> 
  add_markers(x = ~x.tail, y = ~y.tail) |> 
  layout(yaxis = list(
                      scaleanchor = "x",
                      scaleratio = 1))
```

```{r}
d2 <- 
  d1 |> 
  filter(tsec >= 14 & tsec <= 14.5) |> 
  ungroup() |> 
  mutate(outline_even = purrr::map(outline, 
                                   \(out) get_midline(out, xctr,yctr,side,
                                                      spar = 0.3, npt = 26)))
```

```{r}
d2 |> 
  unnest(outline_even) |> 
  group_by(camera, imnum) |> 
  plot_ly() |> 
  add_lines(x = ~xL, y = ~yL, name = "left") |> 
  add_lines(x = ~xR, y = ~yR, name = "right") |> 
  add_lines(x = ~xmid, y = ~ymid, name = "mid") |> 
  layout(yaxis = list(
    scaleanchor = "x",
    scaleratio = 1))

```

## Target to get the midlines

```{targets get_midlines}
target_save_data <- function(fn, data)
{
  fullname <- here(processeddatapath, "midlinedata.parquet")
  arrow::write_parquet(data, fullname)
  fullname
}

list(
  tar_interactive(tar_target(outlinedata1, tar_read(outlinedata1, branches = 1))),
  tar_target(outlinedata_even, 
             outlinedata1 |>   
               ungroup() |> 
               slice_head(n = tar_toggle(20, Inf)) |> 
               mutate(outline_even = purrr::map(outline, 
                                                \(out) get_midline(out, xctr,yctr,side,
                                                                   spar = 0.3, npt = 26),
                                                .progress = "Getting midlines...")),
             pattern = map(outlinedata1)
  ),
  tar_target(midlinedata,
             outlinedata_even |> 
               select(-c(x.head,y.head,x.tail,y.tail, outline)) |> 
               unnest(outline_even),
             pattern = map(outlinedata_even)
  ),
  
  # save the output in a parquet file
  tar_target(midlinedatafile, 
             target_save_data("midlinedata.parquet", midlinedata),
             format = "file")
)
```

  